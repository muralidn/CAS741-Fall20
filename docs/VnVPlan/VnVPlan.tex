\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}


\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{float}
\usepackage{siunitx}
\usepackage{pdflscape}
\usepackage{afterpage}

\usepackage{longtable}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{adjustbox}

\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[square,numbers]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{Naveen Ganesh Muralidharan}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
28-Oct-20 1 & 1.0 & The first draft of the VnV plan\\
14-Dec-20 1 & 2.0 & This version contains the following changes,
\begin{itemize}
\item Changed scope to PD controller.
\item Incorporated Dr.Smith's and reviewers' feedback.
\item Added more test cases.
\end{itemize}
\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  API & Application Program Interface\\
  DCCC & Data and Control Coupling\\
  N & No\\
  PDF & Portable Document Format \\
  PEP8 & Python Enhancement Proposal 8\\
  stdin & Standard input stream\\
  stderr & Standard error stream\\
  stdout & Standard output stream\\
  T & Test\\
  VnV & Verification and Validation\\
  Y & Yes\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

All the units, abbreviations, and symbols recorded in the Software Requirement 
Specification \cite{SRS} apply to this document as well.

\newpage

\pagenumbering{arabic}

This document encompasses the Verification and Validation (VnV) plan for the
\progname{} software. 

Section 3 of this document sets the context for the VnV plan. Section 4  provides
a high-level overview of the VnV plan. Sections 5 and 6 contain the systems and 
unit test cases respectively. 

\wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

The software under test is the simulation of a PD control loop. The functions of 
the PD control loop are,

\begin{itemize}
\item Calculating the Error Signal. Error Signal is the difference between the User 
Input (Set-Point) and the output of the Power Plant (Process-Variable).
\item Computing the output of the PD Controller.
\item Computing the response of the Power-Plant.
\end{itemize}

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}
  
The objectives of the Verification and Validation procedures are to,

\begin{itemize}
\item Establish confidence in software correctness.
\item Ensuring that the software meets the expected quality standards.
\end{itemize}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (MG, MIS, etc.)}

The requirements for the PD Controller software are captured in the Software
Requirements Specification \cite{SRS}

The software design information is captured in the Module Guide  \cite{MG}
and Module Interface Specification \cite{MIS} documents respectively.

\section{Plan}
	
\subsection{Verification and Validation Team}

\wss{You, your classmates and the course instructor.  Maybe your supervisor.}

The members of the VnV team for this project are listed in Table-\ref{tab:VnVTeam}.

\begin{table}[]
\begin{tabular}{cc}
\hline
\multicolumn{1}{c|}{Team   Member} & Role \\ \hline
Naveen Ganesh Muralidharan      & Author    \\
Dr. Spencer Smith                     & Course Instructor and Domain Expert    \\
Ting-Yu Wu                               & Domain Expert    \\
Siddharth (Sid) Shinde               & Secondary reviewer - SRS    \\
Gabriela Sánchez Díaz	               & Secondary reviewer - VnV Plan \\   
\end{tabular}
\caption{Verification and Validation Team}
\label{tab:VnVTeam}
\end{table}

\subsection{SRS Verification Plan}

\wss{List any approaches you intend to use for SRS verification.  This may just
  be ad hoc feedback from reviewers, like your classmates, or you may have
  something more rigorous/systematic in mind..}
  
The SRS will be independently peer-reviewed by members of the VnV team, specifically,
Dr. Spencer Smith, Ting-Yu Wu, and Siddharth (Sid) Shinde.
 
Any issues identified during the review are tracked and verified in Github \cite{Github}.
 
\subsection{Design Verification Plan}

\wss{Plans for design verification}

The software for this project is auto-generated by Drasil Software \cite{Drasil}. Therefore
manual verification of the design is not required.

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}
  
The implemented software is tested as follows,

\begin{itemize}
\item Automated systems testing, where the corresponding test cases
are listed in section \ref{sec_systems_tests}. 
\item Automated unit testing, where the corresponding test cases are
listed in section \ref{sec_unit_tests}. 
\item Statement coverage check. 
\item Static code analysis.
\end{itemize}

\wss{In this section you would also give any details of any plans for static verification of
  the implementation.  Potential techniques include code walk throughs, code
  inspection, static analyzers, etc.}

\subsection{Automated Testing and Verification Tools} \label{sec_Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}
\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

The tools utilized for verification are listed below,

\begin{itemize}
\item Systems Testing - Pytest \cite{Pytest} will be used for automated systems
testing. Since this  is a black-box test, Pytest will be used at the stdin, stdout, and
stderr levels.

\item Unit Testing - Pytest \cite{Pytest} will be used for automated unit testing.
 Since this is a white-box test, Pytest will be used at the API level.

\item Statement coverage - PyTest-Cov \cite{PyTest-Cov}. PyTest-Cov is used 
along with Pytest to obtain the statement coverage.

\item Memory leaks - Valgrind \cite{Valgrind} will  be used for memory leak analyses. 

\item Linting - Flake8 \cite{Flake8}. Linting tool that checks for the coding style against
the PEP8 standard.

\end{itemize}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

There are no plans for the Validation of the \progname{} software.

\section{System Test Description} \label{sec_systems_tests}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

This section contains the systems test cases for the functional requirements in the
SRS \cite{SRS}. The test cases are organized into two categories, the input-output 
tests and simulation parameters test.

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Input-Output tests}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
  
This section verifies section 4.2.6, and the functional requirements of section 5.1
in the SRS \cite {SRS}. Various inputs are provided to the Software Under Test, 
and the output is verified. 
		
\paragraph{Input Constraints test}

\begin{table}[]
\begin{tabular}{lccccccc}
\hline
\multicolumn{1}{l|}{}   & \multicolumn{5}{c|}{Input}                            & \multicolumn{2}{c}{Output} \\ \hline
\multicolumn{1}{c|}{ID} & ${r_{\text{t}}}$ & ${K_{\text{p}}}$  & ${K_{\text{d}}}$ & ${t_{\text{step}}}$ & \multicolumn{1}{c|}{${t_{\text{sim}}}$} & ${y_{\text{t}}}$     & Error Message    \\ \hline
TC-PD-1-1 & 1       & 20      & 1       & 0.01   & 10      & 0.9524 & N/A        \\
TC-PD-1-2 & -0.0001 & 20      & 1       & 0.01   & 10      & N/A    & InputError \\
TC-PD-1-3 & 1       & -0.0001 & 1       & 0.01   & 10      & N/A    & InputError \\
TC-PD-1-4 & 1       & 20      & -0.0001 & 0.01   & 10      & N/A    & InputError \\
TC-PD-1-5 & 1       & 20      & 1       & 0.0009 & 10      & N/A    & InputError \\
TC-PD-1-6 & 1       & 20      & 1       & 10 & 10      & N/A    & InputError \\
TC-PD-1-7 & 1       & 20      & 1       & 0.01   & 0.9999  & N/A    & InputError \\
TC-PD-1-8 & 1       & 20      & 1       & 0.01   & 60.0001 & N/A    & InputError
\end{tabular}
\caption{TC-PD-1 - Input constraints tests}
\label{tab:tc-PD-1}
\end{table}

\begin{itemize}
\item{TC-PD-1\\}
\begin{itemize}

\item{\textbf{Control:}} Automatic
					
\item{\textbf{Initial State:}} None
					
\item{\textbf{Input:}} Set the inputs to the values specified in the `Input' columns 
in Table-\ref{tab:tc-PD-1}.
					
\item{\textbf{Output:}}  Verify that the outputs of the software match the 
values specified in the `Output' columns of Table-\ref{tab:tc-PD-1} within the
allowable margin of error.

\wss{The expected result for the given inputs}

\item{\textbf{Requirement ID(s):}}  FR: Input-Values, FR: Verify-Input-Values, FR: Calculate-Values, FR: Output-Values.

\item{\textbf{Test Case Derivation:}}  This test case is to test the behaviour of the system 
when the system is supplied with inputs that are outside the physical constraints, as specified in Table-4
in the SRS \cite {SRS}. In the test cases TC-PD-1-2 to TC-PD-1-7, the system should produce an InputError, as
the values supplied are beyond the physical constraints of the input signals.

The output, ${y_{\text{t}}}$ refers to the last value of the output list.

The output specified in TC-PD-1-1 has been independently verified using a Simulink model (\cite{Simulink}, 
\cite{PD_Controller}). A relative error of 5\% is applied to accommodate rounding off errors,
and floating-point representation errors between the two software.

\wss{Justify the expected value given in the Output field}
					
\item{\textbf{How the test will be performed:}}  The test will be automated with Pytest as mentioned in 
section \ref{sec_Tools}. 
					
\end{itemize}
\end{itemize}

\paragraph{Output test}

\begin{itemize}
\item{TC-PD-2\\}
\begin{itemize}

\begin{table}[]
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{l|}{}   & \multicolumn{5}{c|}{Input}                            & \multicolumn{1}{c}{Output} \\ \hline
\multicolumn{1}{c|}{ID} & ${r_{\text{t}}}$ & ${K_{\text{p}}}$  & ${K_{\text{d}}}$ & ${t_{\text{step}}}$ & \multicolumn{1}{c|}{${t_{\text{sim}}}$} & ${y_{\text{t}}}$ \\ \hline
TC-PD-2-1 & 20    & 10      & 1       & 0.01   & 10      &18.18 \\
TC-PD-2-2 & 20	  &  5      & 1       & 0.01   & 10      & 16.67     \\
TC-PD-2-3 & 20   & 10    & 15    & 0.01   & 10      & 18.19     \\
TC-PD-2-4 & 20   & 10   & 1     & 0.01   & 5      & 18.17      \\
\end{tabular}
\caption{TC-PD-2 - Output values}
\label{tab:tc-PD-2}
\end{table}


\item{\textbf{Control:}} Automatic
					
\item{\textbf{Initial State:}} None
					
\item{\textbf{Input:}} Set the inputs to the values specified in the `Input' columns 
in Table-\ref{tab:tc-PD-2}.
					
\item{\textbf{Output:}}  Verify that the outputs of the software match the value specified in the `Output' 
column of Table-\ref{tab:tc-PD-2} within the allowable margin of error.

\item{\textbf{Requirement ID(s):}}  FR: Input-Values, FR: Verify-Input-Values, FR: Calculate-Values, FR: Output-Values.

\item{\textbf{Test Case Derivation:}}  This test case is to prove that each input to the software uniquely affects the output
of the software.

The output, ${y_{\text{t}}}$ refers to the last value of the output list.

The outputs have been independently verified using a Simulink model (\cite{Simulink}, 
\cite{PD_Controller}). A relative error of 5\% is applied to accommodate rounding off errors,
and floating-point representation errors between the two software.

\wss{Justify the expected value given in the Output field}
					
\item{\textbf{How the test will be performed:}}  The test will be automated with Pytest as mentioned in 
section \ref{sec_Tools}. 

\end{itemize}
\end{itemize}

...
\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

This section contains the test cases for the non-functional requirements (section
5.2) of the SRS \cite{SRS}.

\wss{Tests related to usability could include conducting a usability test and
  survey.}

\subsubsection{Portability Test}

\begin{itemize}
\item{TC-PD-3\\}
\begin{itemize}

\item{\textbf{Type:}} Semi-Automated, Dynamic.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Portable
					
\item{\textbf{Input/Condition:}}  Execute TC-PD-1 and TC-PD-2 in each of the following 
operating systems,

\begin{enumerate}
\item Windows 10.
\item Bodhi Linux 5.1.0. 
\end {enumerate}
					
\item{\textbf{Output/Result:}} Verify that on each of the operating system, the test 
cases TC-PD-1 and TC-PD-2 passes. 

\item{\textbf{How the test will be performed:}}  On each of the Operating systems,
execute the functional test suite using  Pytest \cite{Pytest}. 

\end{itemize}
\end{itemize}


\subsubsection{Maintainability tests}

\paragraph{Modularity Test}

\paragraph{Data Coupling and Control Coupling (DCCC) Analysis}

The following tests the Data and Control Coupling between the modules. Proving Data Coupling indicates that
the modules are modular. 

\paragraph{Data Coupling}
\begin{itemize}
\item{TC-PD-12\\}
\begin{itemize}
\item{\textbf{Type:}} Manual, Inspection.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}}  NFR: Maintainable
					
\item{\textbf{Input/Condition:}}  Execute TC-PD-1. 
					
\item{\textbf{Output/Result:}} Analyze the data coupling between the modules.

\item{\textbf{How the test will be performed:}}  TC-PD-1 is executed with Pytest. The generated log file (log.txt)
is examined for data coupling.
\end{itemize}
\end{itemize}

\paragraph{Control Coupling}
\begin{itemize}
\item{TC-PD-13\\}
\begin{itemize}
\item{\textbf{Type:}} Manual, Inspection.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Maintainable
					
\item{\textbf{Input/Condition:}}  Execute TC-PD-1. 
					
\item{\textbf{Output/Result:}} Analyze the control coupling between the modules.

\item{\textbf{How the test will be performed:}}  TC-PD-1 is executed with Pytest. The generated log file (log.txt)
is examined for control coupling.
\end{itemize}
\end{itemize}

\paragraph{Linting}

\begin{itemize}
\item{TC-PD-5\\}
\begin{itemize}

\item{\textbf{Type:}} Static, Automated.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Maintainable
					
\item{\textbf{Input/Condition:}}  The source code files.
					
\item{\textbf{Output/Result:}} Verify that the source code does not contain any PEP-8 violations.

\item{\textbf{How the test will be performed:}}   This test will be automated using the 
Flake8 \cite{Flake8} tool.

\end{itemize}
\end{itemize}

\paragraph{Documented}
\begin{itemize}
\item{TC-PD-6\\}
\begin{itemize}
\item{\textbf{Type:}} Inspection
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Maintainable
					
\item{\textbf{Input/Condition:}}  The PDF API reference generated with Doxygen.
					
\item{\textbf{Output/Result:}} Verify that all the classes, methods and modules in the source
code are documented in the API reference.

\item{\textbf{How the test will be performed:}}  
\begin{enumerate}
\item Run `make doc` to generate the Doxygen Latex files.
\item Navigate to the Latex directory and run `make' to generate the Doxygen PDF file.
\item Inspect the PDF file, ensure all the functions in the source code are adequately documented. 
\end {enumerate}
\end{itemize}
\end{itemize}

\subsubsection{Security Tests}

\paragraph{Memory leak check}
\begin{itemize}
\item{TC-PD-7\\}
\begin{itemize}
\item{\textbf{Type:}} Automated, Dynamic.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Secure
					
\item{\textbf{Input/Condition:}}  Execute the test case of TC-PD-1-1.
					
\item{\textbf{Output/Result:}} Verify that the test case TC-PD-1-1 passes and there
are no memory leaks identified in the  report generated by Valgrind.

\item{\textbf{How the test will be performed:}} Valgrind \cite{Valgrind} is used to fork
the python script. After the execution, a report is generated by Valgrind. 
\end{itemize}
\end{itemize}

\paragraph{Divide by-zero check}
\begin{itemize}
\item{TC-PD-8\\}
\begin{itemize}
\item{\textbf{Type:}} Static, Inspection.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}}  NFR: Secure
					
\item{\textbf{Input/Condition:}}  Source code of the PD controller.
					
\item{\textbf{Output/Result:}} For every division in the source code, verify that
the denominator is tested for a non-zero value. 

\item{\textbf{How the test will be performed:}}  Manual inspection of the source code.
\end{itemize}
\end{itemize}

---

\paragraph{Negative square root check}
\begin{itemize}
\item{TC-PD-9\\}
\begin{itemize}
\item{\textbf{Type:}} Static analysis and Inspection
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}}  NFR: Secure
					
\item{\textbf{Input/Condition:}}  Source code of the PD controller.
					
\item{\textbf{Output/Result:}} For every square root function in the source code,
verify that the operands are tested for negative values before the function call.

\item{\textbf{How the test will be performed:}}  Manual inspection of the source code.
\end{itemize}
\end{itemize}

----

\subsubsection{Verifiability test}

\paragraph{Traceability}

\begin{itemize}
\item{TC-PD-10\\}
\begin{itemize}
\item{\textbf{Type:}} Inspection
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Verifiable
					
\item{\textbf{Input/Condition:}} VnV report and the SRS of the PD Controller.
					
\item{\textbf{Output/Result:}} Verify that for each requirement in the SRS, there
exists at least one test case in the VnV report.
 
\item{\textbf{How the test will be performed:}} Manual inspection of the documents.
\end{itemize}
\end{itemize}

\paragraph{Statement Coverage Test}

\begin{itemize}
\item{TC-PD-11\\}
\begin{itemize}

\item{\textbf{Type:}} Dynamic, Automated.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}} NFR: Verifiable
					
\item{\textbf{Input/Condition:}}  Execute test cases TC-PD-1 and TC-PD-2.
					
\item{\textbf{Output/Result:}} Verify that the statement coverage is 100\%.

\item{\textbf{How the test will be performed:}}  This test will be executed with Pytest \cite{Pytest} and 
the Pytest-Cov \cite{PyTest-Cov}. The tests should be executed with the `--cov' command-line option.

\end{itemize}
\end{itemize}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

Table-\ref{tblTrace} contains the mapping of requirements to test cases.
  
\begin{table}[]
\begin{adjustbox}{width=1.29\textwidth}
\begin{tabular}{c|c|c|c|c|c|c|c|c|}
 & Input-Values & Verify-Input-Values & Calculate-Values & Output-Values & Portable & Secure & Maintainable & Verifiable \\ \hline
TC-PD-1 & X & X & X  & X  &  &  &  &  \\
TC-PD-2 & X  & X  & X & X &  &  &  &  \\
TC-PD-3  &  &  &  &   & X  &  &  &  \\
TC-PD-5  &  &  &  &  &  &  &X  &  \\
TC-PD-6  &  &  &  &  &  &  &X  &  \\
TC-PD-7  &  &  &  &  &  &X  &  &  \\
TC-PD-8  &  &  &  &  &  &X  &  &  \\
TC-PD-9  &  &  &  &  &  &X  &  &  \\
TC-PD-10 &  &  &  &  &  &  &  &X  \\
TC-PD-11 &  &  &  &  &  &  &  &X \\
TC-PD-12 &  &  &   &   &  &  &  &X  \\
TC-PD-13 &   &  &  &  &  &  &  &X \\
\end{tabular}
\end{adjustbox}
\caption{Requirements vs Test Cases Trace Matrix}
\label{tblTrace}
\end{table}

\section{Unit Test Description}  \label{sec_unit_tests}

\wss{Reference your MIS and explain your overall philosophy for test case
  selection.}  
\wss{This section should not be filled in until after the MIS has
  been completed.}

The source code for the \progname{} has been auto-generated by the 
Drasil software (\cite{Drasil}). The generated software contains the following modules,

\begin{itemize}

\item Calculations.py  - Provides functions for calculating the outputs.

\item Constants.py  - Provides the structure for holding constant values.

\item Control.py  - Controls the flow of the program.

\item InputParameters.py - Provides the function for reading inputs and the function 
for checking the physical constraints on the input.

\item OutputFormat.py  -  Provides the function for writing outputs.

\end{itemize}

Automated unit-testing will be performed on select modules.

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

The following areas were tested as part of the Systems testing,

\begin{itemize}
\item Software correctness and physical constraints were tested in test cases TC-PD-1 and TC-PD-2.
\item Statement coverage of 100\% was tested in test cases TC-PD-11.
\item Coupling was tested in TC-PD-12 and TC-PD-13.
\end{itemize}

These tests adequately prove the correctness of the modules and the dataflow between them. Therefore the only 
additional testing that is necessary for the analysis of the Step Time (${t_{\text{step}}}$),
and Simulation Time (${t_{\text{sim}}}$). For this, the Calculations.py module will be unit tested.

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Calculations.py}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

This section tests for the Step Time (${t_{\text{step}}}$) through automated unit testing.

\begin{itemize}
\item{TC-PD-11\\}
\begin{itemize}

\item{\textbf{Type:}}  Automated, Black Box.
					
\item{\textbf{Initial State:}} None

\item{\textbf{Requirement ID(s):}}  FR: Calculate-Values
					
\item{\textbf{Input/Condition:}}  Set the input as follows,

\begin{itemize}
\item ${r_{\text{t}}}$ = 1
\item ${K_{\text{p}}}$ = 10
\item ${K_{\text{d}}}$ = 1
\item ${t_{\text{step}}}$ = 0.01
\item ${t_{\text{sim}}}$ = 1 
\end{itemize}

\item{\textbf{Output/Result:}} Verify that the length of the list ${y_{\text{t}}}$ is 101.

\item{\textbf{Test Case Derivation:}} The ODE is integrated every  ${t_{\text{step}}}$ seconds 
until  ${t_{\text{sim}}}$ seconds. Therefore the no of samples will be  
((${t_{\text{sim}}}$/ ${t_{\text{step}}}$) + 1), where 1 is for the initial value.

\item{\textbf{How the test will be performed:}}  This test will be executed with Pytest \cite{Pytest}.

\end{itemize}
\end{itemize}

					
\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
Table-\ref{tblTrace2} contains the mapping of requirements to modules.
  
\begin{table}[]
\begin{adjustbox}{width=1.29\textwidth}
\begin{tabular}{c|c|c|c|c|c|c|c|c|}
 & Input-Values & Verify-Input-Values & Calculate-Values & Output-Values & Portable & Secure & Maintainable & Verifiable \\ \hline
Calculations.py	      &  &  & X  &   &  &  &  &  \\
Constants.py	      &  &  & X  &   &  &  &  &  \\
Control.py		      &X  &  &  &   &  &  &  &  \\
InputParameters.py  &X  &X  &  &   &  &  &  &  \\
OutputFormat.py &   &  &  &X   &  &  &  &  \\
\end{tabular}
\end{adjustbox}
\caption{Requirements vs Modules Trace Matrix}
\label{tblTrace2}
\end{table}


\bibliographystyle{abbrvnat}
\bibliography {../../refs/References}

%\newpage

%\section{Appendix}

%This is where you can place additional information.

%\subsection{Symbolic Parameters}

%The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
%Their values are defined in this section for easy maintenance.

%\subsection{Usability Survey Questions?}

%\wss{This is a section that would be appropriate for some projects.}

\end{document}
